{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5479cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thush\\anaconda3\\envs\\DILINI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Oct/2023 10:32:41] \"POST /get_prediction HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the OpenAI GPT-3 API endpoint and your API key\n",
    "api_endpoint = \"https://api.openai.com/v1/completions\"\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up request headers for OpenAI GPT-3\n",
    "request_headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer ' + \"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\"\n",
    "}\n",
    "\n",
    "# Define the model and tokenizer paths for sentiment analysis (adjust the paths as needed)\n",
    "model_path = \"bert-base-uncased\"  # e.g., \"bert_model/\"\n",
    "tokenizer_path = \"bert-base-uncased\"  # e.g., \"bert_tokenizer/\"\n",
    "\n",
    "# Load the pre-trained model and tokenizer for sentiment analysis\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Function to predict sentiment label from news text\n",
    "def predict_sentiment(news_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(news_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted probabilities (logits)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Get the predicted label (0 or 1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return predicted_label, probabilities\n",
    "\n",
    "@app.route('/get_prediction', methods=['POST'])\n",
    "def get_prediction():\n",
    "    try:\n",
    "        # Get the input text from the JSON request data\n",
    "        input_text = request.json.get('input_text')\n",
    "\n",
    "        if input_text is not None:\n",
    "            # Predict sentiment label and probabilities\n",
    "            predicted_label, probabilities = predict_sentiment(input_text)\n",
    "\n",
    "            # Convert the probabilities to Python floats\n",
    "            probabilities = probabilities.tolist()[0]\n",
    "\n",
    "            # Configuring prompt for GPT-3\n",
    "            prompt = f'Generate a financial advice (buy, sell,hold) for this news article({input_text}), Using this Predicted Label({predicted_label})(if this 0 = no affect in stock price, if 1 = affect in stock price) and Predicted Probabilities ({probabilities[0]:.2f}, {probabilities[1]:.2f})'\n",
    "\n",
    "            # Create the request data to send to the OpenAI API\n",
    "            request_data = {\n",
    "                'model': 'text-davinci-003',  # You can use the appropriate GPT-3 model here\n",
    "                'prompt': prompt,\n",
    "                'max_tokens': 1000  # You can adjust the max tokens as needed\n",
    "            }\n",
    "\n",
    "            # Make a POST request to the OpenAI API\n",
    "            response = requests.post(api_endpoint, headers=request_headers, json=request_data)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Get the generated text from the OpenAI response\n",
    "                generated_text = response.json()['choices'][0]['text']\n",
    "\n",
    "                # Return the generated text, predicted_label, and probabilities as a single string\n",
    "                result = f\"Generated Text: {generated_text}\\nPredicted Label: {predicted_label}\\nPredicted Probabilities: {probabilities[0]:.2f}, {probabilities[1]:.2f}\"\n",
    "                return result\n",
    "            else:\n",
    "                # Handle API response errors\n",
    "                return f\"Request Failed with status code: {response.status_code}\"\n",
    "        else:\n",
    "            # Handle missing input_text\n",
    "            return \"Missing input_text in the request\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        return str(e)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c845ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Using cached flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from flask)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\thush\\anaconda3\\envs\\dilini\\lib\\site-packages (from flask) (3.1.2)\n",
      "Collecting itsdangerous>=2.1.2 (from flask)\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.1.3 (from flask)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting blinker>=1.6.2 (from flask)\n",
      "  Using cached blinker-1.6.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\thush\\anaconda3\\envs\\dilini\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thush\\anaconda3\\envs\\dilini\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.1)\n",
      "Using cached flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "Using cached blinker-1.6.3-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: Werkzeug, itsdangerous, click, blinker, flask\n",
      "Successfully installed Werkzeug-3.0.1 blinker-1.6.3 click-8.1.7 flask-3.0.0 itsdangerous-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdc0889",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4134312529.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    export OPENAI_API_KEY=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\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export OPENAI_API_KEY=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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4699656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
