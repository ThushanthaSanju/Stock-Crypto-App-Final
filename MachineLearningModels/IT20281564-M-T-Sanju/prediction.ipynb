{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb80cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2023-04-27  105.230003  109.150002  104.419998  108.370003  108.370003   \n",
      "2023-04-28  107.800003  108.290001  106.040001  108.220001  108.220001   \n",
      "2023-05-01  107.720001  108.680000  107.500000  107.709999  107.709999   \n",
      "2023-05-02  107.660004  107.730003  104.500000  105.980003  105.980003   \n",
      "2023-05-03  106.220001  108.129997  105.620003  106.120003  106.120003   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2023-10-23  136.229996  139.020004  135.110001  137.899994  137.899994   \n",
      "2023-10-24  139.160004  140.710007  138.750000  140.119995  140.119995   \n",
      "2023-10-25  129.770004  130.100006  126.089996  126.669998  126.669998   \n",
      "2023-10-26  124.470001  125.459999  122.320000  123.440002  123.440002   \n",
      "2023-10-27  124.029999  124.440002  121.459999  123.400002  123.400002   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2023-04-27  38235200  \n",
      "2023-04-28  23957900  \n",
      "2023-05-01  20926300  \n",
      "2023-05-02  20343100  \n",
      "2023-05-03  17116300  \n",
      "...              ...  \n",
      "2023-10-23  20780700  \n",
      "2023-10-24  26535200  \n",
      "2023-10-25  58796100  \n",
      "2023-10-26  33907400  \n",
      "2023-10-27  37349000  \n",
      "\n",
      "[128 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the stock symbol and date range\n",
    "companyTicker = \"GOOG\"\n",
    "symbol = companyTicker\n",
    "start_date = pd.Timestamp.now() - pd.DateOffset(days=200)\n",
    "end_date = pd.Timestamp.now()\n",
    "\n",
    "# Fetch the historical data\n",
    "df = yf.download(symbol, start=start_date, end=end_date)\n",
    "df = df.iloc[-128:]\n",
    "# Add the \"Date\" column to the DataFrame\n",
    "# df['Date'] = df.index\n",
    "\n",
    "# Print the fetched data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e5dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config\n",
    "    \n",
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k,\n",
    "                           input_shape=input_shape,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k,\n",
    "                         input_shape=input_shape,\n",
    "                         kernel_initializer='glorot_uniform',\n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v,\n",
    "                           input_shape=input_shape,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n",
    "\n",
    "            # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7\n",
    "            self.linear = Dense(input_shape[0][-1],\n",
    "                                input_shape=input_shape,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7\n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1)\n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer\n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29824561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(df):\n",
    "#     df.drop(columns=['Date'], inplace=True)\n",
    "    x = df[:128]\n",
    "    x = x.values\n",
    "    x = x.reshape((1, 128, 5))\n",
    "    print(x.shape)\n",
    "    model = tf.keras.models.load_model(f\"{companyTicker}.hdf5\",\n",
    "                                   custom_objects={'Time2Vector': Time2Vector,\n",
    "                                                   'SingleAttention': SingleAttention,\n",
    "                                                   'MultiAttention': MultiAttention,\n",
    "                                                   'TransformerEncoder': TransformerEncoder})\n",
    "    pred = model.predict(x) \n",
    "    print(pred)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7449cf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-10-23</th>\n",
       "      <td>136.229996</td>\n",
       "      <td>139.020004</td>\n",
       "      <td>135.110001</td>\n",
       "      <td>137.899994</td>\n",
       "      <td>20780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-24</th>\n",
       "      <td>139.160004</td>\n",
       "      <td>140.710007</td>\n",
       "      <td>138.750000</td>\n",
       "      <td>140.119995</td>\n",
       "      <td>26535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-25</th>\n",
       "      <td>129.770004</td>\n",
       "      <td>130.100006</td>\n",
       "      <td>126.089996</td>\n",
       "      <td>126.669998</td>\n",
       "      <td>58796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26</th>\n",
       "      <td>124.470001</td>\n",
       "      <td>125.459999</td>\n",
       "      <td>122.320000</td>\n",
       "      <td>123.440002</td>\n",
       "      <td>33907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-27</th>\n",
       "      <td>124.029999</td>\n",
       "      <td>124.440002</td>\n",
       "      <td>121.459999</td>\n",
       "      <td>123.400002</td>\n",
       "      <td>37349000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume\n",
       "Date                                                                \n",
       "2023-10-23  136.229996  139.020004  135.110001  137.899994  20780700\n",
       "2023-10-24  139.160004  140.710007  138.750000  140.119995  26535200\n",
       "2023-10-25  129.770004  130.100006  126.089996  126.669998  58796100\n",
       "2023-10-26  124.470001  125.459999  122.320000  123.440002  33907400\n",
       "2023-10-27  124.029999  124.440002  121.459999  123.400002  37349000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('IBM.csv', delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "# ClosingPrice = make_predict(df)\n",
    "\n",
    "# Use the delimiter and columns when reading the data\n",
    "selected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df = df[selected_columns]\n",
    "\n",
    "\n",
    "# Replace 0 to avoid dividing by 0 later on\n",
    "# df['Volume'].replace(to_replace=0, method='ffill', inplace=True)\n",
    "# df.sort_values('Date', inplace=True)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838893b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 5)\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "[[0.3746192]]\n",
      "0.3746192\n"
     ]
    }
   ],
   "source": [
    "ClosingPrice = make_predict(df)\n",
    "print(ClosingPrice[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495ac431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Close Price: 105.20999908447266\n",
      "Maximum Close Price: 141.6999969482422\n"
     ]
    }
   ],
   "source": [
    "# Calculate the minimum and maximum values of the \"Close\" column\n",
    "min_close = df['Close'].min()\n",
    "max_close = df['Close'].max()\n",
    "\n",
    "print(\"Minimum Close Price:\", min_close)\n",
    "print(\"Maximum Close Price:\", max_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df6b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.88\n"
     ]
    }
   ],
   "source": [
    "original_close = (ClosingPrice[0,0] * (max_close - min_close)) + min_close\n",
    "price = round(original_close, 2)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4043dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [130.53, 115.25, 119.57, 124.24, 135.35]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the range and the specific value to multiply by\n",
    "min_value = 0.95  # Minimum value in the range\n",
    "max_value = 1.15  # Maximum value in the range\n",
    "specific_value = price  # Value to multiply by\n",
    "num_iterations = 5  # Number of iterations\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Create a for loop to perform the specified number of iterations\n",
    "for _ in range(num_iterations):\n",
    "    # Generate a random number within the specified range\n",
    "    random_number = random.uniform(min_value, max_value)\n",
    "\n",
    "    # Multiply the random number by the specific value\n",
    "    result = round(random_number * specific_value,2)\n",
    "\n",
    "    # Append the result to the list\n",
    "    results.append(result)\n",
    "\n",
    "# Print the list of results\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9301fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the predicted stock price of 118.88 for the ticker GOOG, it is advised to buy the stock as it is predicted to increase in value. Research areas to take decision include macroeconomic analysis, technical analysis, fundamental analysis, and market sentiment analysis. Additionally, research into the company's financials, management, and competitive landscape can help to inform decision making.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re \n",
    "\n",
    "    #  chat gpt implementation\n",
    "\n",
    "api_endpoint = \"https://api.openai.com/v1/completions\"\n",
    "api_key =\"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\"\n",
    "\n",
    "    # Headers configuration\n",
    "request_headers = {\n",
    "        'Content-Type' : 'application/json',\n",
    "        'Authorization': 'Bearer ' + api_key\n",
    "    }\n",
    "\n",
    "    # Configuring prompt\n",
    "request_data= {\n",
    "        'model' : 'text-davinci-003',\n",
    "        'prompt' : \"mention research areas also Here is the predicted Stock price \"+str(price)  + \"for the ticker \" + companyTicker + \"generate a stock market advice based on this and tell me the research areas to take decision\",\n",
    "        'max_tokens':100, \n",
    "        'temperature':0.5\n",
    "    }\n",
    "\n",
    "    # API call and storing response\n",
    "response = requests.post(api_endpoint,headers=request_headers,json=request_data)\n",
    "\n",
    "    # if the response is success writing a JSON object\n",
    "if response.status_code == 200:\n",
    "        print(response.json()['choices'][0]['text'])\n",
    "\n",
    "     # if the response is unsuccess printing the status code\n",
    "else:\n",
    "        print(f\"Request Failed with status code: {str(response.status_code)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165adf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
